{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big thanks to Susan Li for the awesome tutorial on LDA\n",
    "[https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24]\n",
    "\n",
    "This Notebook was inspired by the Medium article above and is written by Advith Chegu for HackDown 2020. Some more sources:\n",
    "\n",
    "[https://medium.com/@osas.usen/topic-extraction-from-tweets-using-lda-a997e4eb0985]\n",
    "\n",
    "[https://www.kaggle.com/therohk/million-headlines/data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "DATASET_COLUMNS  = [\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "DATASET_ENCODING = \"ISO-8859-1\"\n",
    "data = pd.read_json('News_Category_Dataset_v2.json', lines=True)\n",
    "data['to_vec'] = data[['category', 'headline', 'short_description']].agg(' '.join, axis=1)\n",
    "data.set_index('headline', drop=True, append=False, inplace=False, verify_integrity=False)\n",
    "data['index'] = data.index\n",
    "documents = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200853\n",
      "           authors       category       date  \\\n",
      "0  Melissa Jeltsen          CRIME 2018-05-26   \n",
      "1    Andy McDonald  ENTERTAINMENT 2018-05-26   \n",
      "2       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
      "3       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
      "4       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
      "\n",
      "                                            headline  \\\n",
      "0  There Were 2 Mass Shootings In Texas Last Week...   \n",
      "1  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
      "2    Hugh Grant Marries For The First Time At Age 57   \n",
      "3  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
      "4  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
      "\n",
      "                                                link  \\\n",
      "0  https://www.huffingtonpost.com/entry/texas-ama...   \n",
      "1  https://www.huffingtonpost.com/entry/will-smit...   \n",
      "2  https://www.huffingtonpost.com/entry/hugh-gran...   \n",
      "3  https://www.huffingtonpost.com/entry/jim-carre...   \n",
      "4  https://www.huffingtonpost.com/entry/julianna-...   \n",
      "\n",
      "                                   short_description  \\\n",
      "0  She left her husband. He killed their children...   \n",
      "1                           Of course it has a song.   \n",
      "2  The actor and his longtime girlfriend Anna Ebe...   \n",
      "3  The actor gives Dems an ass-kicking for not fi...   \n",
      "4  The \"Dietland\" actress said using the bags is ...   \n",
      "\n",
      "                                              to_vec  index  \n",
      "0  CRIME There Were 2 Mass Shootings In Texas Las...      0  \n",
      "1  ENTERTAINMENT Will Smith Joins Diplo And Nicky...      1  \n",
      "2  ENTERTAINMENT Hugh Grant Marries For The First...      2  \n",
      "3  ENTERTAINMENT Jim Carrey Blasts 'Castrato' Ada...      3  \n",
      "4  ENTERTAINMENT Julianna Margulies Uses Donald T...      4  \n"
     ]
    }
   ],
   "source": [
    "print(len(documents))\n",
    "print(documents[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/advithchegu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "stop_words = ['http']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3 and token not in stop_words and '@' not in token:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['Laura', 'Paddison']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['laura', 'paddison']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = documents[documents['index'] == 4310].values[0][0]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [crime, mass, shoot, texa, week, leav, husband...\n",
       "1    [entertain, smith, join, diplo, nicki, world, ...\n",
       "2    [entertain, hugh, grant, marri, time, actor, l...\n",
       "3    [entertain, carrey, blast, castrato, adam, sch...\n",
       "4    [entertain, julianna, marguli, use, donald, tr...\n",
       "5    [entertain, morgan, freeman, devast, sexual, h...\n",
       "6    [entertain, donald, trump, lovin, mcdonald, ji...\n",
       "7    [entertain, watch, amazon, prime, week, great,...\n",
       "8    [entertain, mike, myer, reveal, like, fourth, ...\n",
       "9    [entertain, watch, hulu, week, get, recent, ac...\n",
       "Name: to_vec, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = documents['to_vec'].map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 america\n",
      "1 children\n",
      "2 crime\n",
      "3 husband\n",
      "4 kill\n",
      "5 leav\n",
      "6 mass\n",
      "7 shoot\n",
      "8 texa\n",
      "9 week\n",
      "10 cours\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(121, 1), (187, 1), (323, 1), (949, 1), (959, 1), (3684, 1), (4346, 1)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 121 (\"impact\") appears 1 time.\n",
      "Word 187 (\"women\") appears 1 time.\n",
      "Word 323 (\"face\") appears 1 time.\n",
      "Word 949 (\"number\") appears 1 time.\n",
      "Word 959 (\"intern\") appears 1 time.\n",
      "Word 3684 (\"handi\") appears 1 time.\n",
      "Word 4346 (\"inevit\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_4310 = bow_corpus[4310]\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                               dictionary[bow_doc_4310[i][0]], \n",
    "bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.28118952113002166),\n",
      " (1, 0.27998207942225095),\n",
      " (2, 0.27737831958734893),\n",
      " (3, 0.3605228811876239),\n",
      " (4, 0.30418734124514596),\n",
      " (5, 0.2856753625347595),\n",
      " (6, 0.4125666899870586),\n",
      " (7, 0.30562807249758883),\n",
      " (8, 0.37797091116476994),\n",
      " (9, 0.23453113858671673)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=40, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LdaModel(num_terms=11070, num_topics=40, decay=0.5, chunksize=2000)\n"
     ]
    }
   ],
   "source": [
    "print(lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.039*\"click\" + 0.038*\"yoga\" + 0.033*\"shop\" + 0.023*\"publish\" + 0.023*\"sit\" + 0.022*\"wine\" + 0.021*\"breath\" + 0.020*\"entir\" + 0.018*\"sexual\" + 0.018*\"blogger\"\n",
      "Topic: 1 \n",
      "Words: 0.065*\"busi\" + 0.023*\"social\" + 0.021*\"compani\" + 0.021*\"tech\" + 0.018*\"bank\" + 0.015*\"collect\" + 0.014*\"memori\" + 0.014*\"impact\" + 0.013*\"network\" + 0.012*\"develop\"\n",
      "Topic: 2 \n",
      "Words: 0.183*\"well\" + 0.157*\"food\" + 0.031*\"stress\" + 0.028*\"holiday\" + 0.016*\"eat\" + 0.015*\"best\" + 0.011*\"tradit\" + 0.010*\"time\" + 0.010*\"way\" + 0.009*\"fast\"\n",
      "Topic: 3 \n",
      "Words: 0.050*\"american\" + 0.034*\"risk\" + 0.021*\"opportun\" + 0.019*\"approach\" + 0.018*\"countri\" + 0.016*\"relat\" + 0.014*\"signific\" + 0.014*\"rat\" + 0.014*\"higher\" + 0.013*\"mayb\"\n",
      "Topic: 4 \n",
      "Words: 0.054*\"guid\" + 0.043*\"father\" + 0.040*\"obama\" + 0.035*\"bride\" + 0.019*\"middleton\" + 0.017*\"british\" + 0.016*\"bear\" + 0.016*\"giant\" + 0.015*\"royal\" + 0.014*\"coast\"\n",
      "Topic: 5 \n",
      "Words: 0.043*\"game\" + 0.040*\"super\" + 0.040*\"season\" + 0.035*\"bowl\" + 0.034*\"natur\" + 0.032*\"play\" + 0.027*\"understand\" + 0.026*\"sport\" + 0.022*\"rise\" + 0.021*\"short\"\n",
      "Topic: 6 \n",
      "Words: 0.056*\"daili\" + 0.041*\"train\" + 0.033*\"chocol\" + 0.029*\"stuff\" + 0.022*\"goal\" + 0.021*\"prepar\" + 0.020*\"passion\" + 0.019*\"clinton\" + 0.018*\"dad\" + 0.017*\"sort\"\n",
      "Topic: 7 \n",
      "Words: 0.053*\"spring\" + 0.041*\"citi\" + 0.036*\"singl\" + 0.029*\"clean\" + 0.021*\"biggest\" + 0.021*\"energi\" + 0.015*\"problem\" + 0.014*\"afford\" + 0.013*\"enjoy\" + 0.012*\"pressur\"\n",
      "Topic: 8 \n",
      "Words: 0.102*\"women\" + 0.046*\"girl\" + 0.024*\"medit\" + 0.020*\"shoe\" + 0.020*\"note\" + 0.019*\"woman\" + 0.018*\"outfit\" + 0.016*\"interest\" + 0.015*\"march\" + 0.014*\"highlight\"\n",
      "Topic: 9 \n",
      "Words: 0.029*\"birthday\" + 0.021*\"budget\" + 0.019*\"invit\" + 0.019*\"spiritu\" + 0.019*\"footbal\" + 0.017*\"jennif\" + 0.016*\"gown\" + 0.016*\"june\" + 0.015*\"doubt\" + 0.015*\"exampl\"\n",
      "Topic: 10 \n",
      "Words: 0.175*\"travel\" + 0.037*\"photo\" + 0.022*\"world\" + 0.017*\"home\" + 0.016*\"news\" + 0.015*\"place\" + 0.013*\"best\" + 0.012*\"park\" + 0.009*\"small\" + 0.009*\"flight\"\n",
      "Topic: 11 \n",
      "Words: 0.120*\"parent\" + 0.031*\"kid\" + 0.027*\"children\" + 0.025*\"thing\" + 0.021*\"know\" + 0.019*\"famili\" + 0.019*\"child\" + 0.018*\"time\" + 0.016*\"year\" + 0.015*\"think\"\n",
      "Topic: 12 \n",
      "Words: 0.065*\"mother\" + 0.022*\"year\" + 0.020*\"death\" + 0.018*\"type\" + 0.015*\"sugar\" + 0.014*\"journey\" + 0.013*\"religion\" + 0.013*\"age\" + 0.012*\"calori\" + 0.011*\"older\"\n",
      "Topic: 13 \n",
      "Words: 0.042*\"cancer\" + 0.040*\"scienc\" + 0.026*\"tip\" + 0.023*\"date\" + 0.023*\"studi\" + 0.023*\"space\" + 0.022*\"brain\" + 0.020*\"research\" + 0.019*\"effect\" + 0.019*\"craft\"\n",
      "Topic: 14 \n",
      "Words: 0.062*\"hotel\" + 0.043*\"plan\" + 0.027*\"polit\" + 0.022*\"vote\" + 0.017*\"anti\" + 0.016*\"support\" + 0.014*\"celeb\" + 0.013*\"group\" + 0.013*\"right\" + 0.012*\"schedul\"\n",
      "Topic: 15 \n",
      "Words: 0.066*\"high\" + 0.066*\"school\" + 0.052*\"question\" + 0.034*\"colleg\" + 0.032*\"student\" + 0.029*\"univers\" + 0.027*\"educ\" + 0.026*\"answer\" + 0.021*\"class\" + 0.019*\"uniqu\"\n",
      "Topic: 16 \n",
      "Words: 0.024*\"pain\" + 0.023*\"beach\" + 0.023*\"advic\" + 0.020*\"wonder\" + 0.020*\"worst\" + 0.019*\"vintag\" + 0.019*\"piec\" + 0.019*\"evolut\" + 0.017*\"habit\" + 0.016*\"makeup\"\n",
      "Topic: 17 \n",
      "Words: 0.038*\"wall\" + 0.035*\"street\" + 0.035*\"presid\" + 0.032*\"fit\" + 0.030*\"addict\" + 0.028*\"christma\" + 0.024*\"trump\" + 0.019*\"romney\" + 0.018*\"america\" + 0.018*\"polit\"\n",
      "Topic: 18 \n",
      "Words: 0.048*\"entertain\" + 0.035*\"recip\" + 0.030*\"star\" + 0.028*\"summer\" + 0.022*\"film\" + 0.021*\"music\" + 0.017*\"movi\" + 0.015*\"celebr\" + 0.013*\"award\" + 0.013*\"video\"\n",
      "Topic: 19 \n",
      "Words: 0.071*\"chang\" + 0.042*\"environ\" + 0.026*\"tumblr\" + 0.026*\"friday\" + 0.025*\"morn\" + 0.022*\"report\" + 0.016*\"general\" + 0.016*\"climat\" + 0.014*\"expens\" + 0.014*\"secur\"\n",
      "Topic: 20 \n",
      "Words: 0.062*\"perfect\" + 0.040*\"water\" + 0.037*\"trend\" + 0.022*\"guest\" + 0.021*\"destin\" + 0.020*\"updat\" + 0.017*\"west\" + 0.017*\"dark\" + 0.014*\"stylish\" + 0.014*\"brand\"\n",
      "Topic: 21 \n",
      "Words: 0.079*\"live\" + 0.066*\"well\" + 0.034*\"home\" + 0.027*\"voic\" + 0.025*\"life\" + 0.020*\"healthi\" + 0.019*\"black\" + 0.015*\"queer\" + 0.014*\"love\" + 0.012*\"feel\"\n",
      "Topic: 22 \n",
      "Words: 0.029*\"case\" + 0.026*\"court\" + 0.024*\"rule\" + 0.023*\"editor\" + 0.017*\"one\" + 0.016*\"locat\" + 0.015*\"injuri\" + 0.015*\"angel\" + 0.014*\"enter\" + 0.012*\"introduc\"\n",
      "Topic: 23 \n",
      "Words: 0.245*\"drink\" + 0.032*\"anim\" + 0.023*\"form\" + 0.019*\"video\" + 0.017*\"depress\" + 0.016*\"treat\" + 0.016*\"googl\" + 0.015*\"youtub\" + 0.015*\"kick\" + 0.012*\"sandi\"\n",
      "Topic: 24 \n",
      "Words: 0.107*\"money\" + 0.041*\"parti\" + 0.023*\"cloth\" + 0.017*\"polit\" + 0.014*\"spend\" + 0.014*\"million\" + 0.013*\"paul\" + 0.013*\"chicken\" + 0.012*\"job\" + 0.010*\"strategi\"\n",
      "Topic: 25 \n",
      "Words: 0.028*\"touch\" + 0.027*\"soul\" + 0.022*\"michell\" + 0.021*\"david\" + 0.021*\"listen\" + 0.018*\"scene\" + 0.018*\"solut\" + 0.016*\"hide\" + 0.015*\"awesom\" + 0.015*\"luxuri\"\n",
      "Topic: 26 \n",
      "Words: 0.038*\"manag\" + 0.032*\"card\" + 0.026*\"cocktail\" + 0.024*\"spirit\" + 0.023*\"thanksgiv\" + 0.020*\"board\" + 0.019*\"jessica\" + 0.019*\"ador\" + 0.018*\"campaign\" + 0.017*\"clip\"\n",
      "Topic: 27 \n",
      "Words: 0.075*\"health\" + 0.042*\"care\" + 0.020*\"product\" + 0.019*\"cost\" + 0.019*\"patient\" + 0.018*\"benefit\" + 0.018*\"medic\" + 0.018*\"cook\" + 0.017*\"doctor\" + 0.014*\"restaur\"\n",
      "Topic: 28 \n",
      "Words: 0.248*\"divorc\" + 0.059*\"marriag\" + 0.027*\"result\" + 0.026*\"sign\" + 0.017*\"beer\" + 0.016*\"split\" + 0.015*\"survey\" + 0.013*\"partner\" + 0.011*\"hill\" + 0.010*\"affair\"\n",
      "Topic: 29 \n",
      "Words: 0.101*\"check\" + 0.036*\"save\" + 0.028*\"decor\" + 0.027*\"london\" + 0.027*\"mental\" + 0.020*\"screen\" + 0.020*\"search\" + 0.018*\"journal\" + 0.018*\"pregnant\" + 0.017*\"lower\"\n",
      "Topic: 30 \n",
      "Words: 0.065*\"cultur\" + 0.060*\"design\" + 0.052*\"art\" + 0.042*\"book\" + 0.018*\"mom\" + 0.016*\"famous\" + 0.016*\"photograph\" + 0.016*\"photo\" + 0.016*\"cold\" + 0.016*\"paint\"\n",
      "Topic: 31 \n",
      "Words: 0.029*\"blue\" + 0.027*\"requir\" + 0.023*\"buy\" + 0.018*\"economi\" + 0.018*\"paper\" + 0.016*\"print\" + 0.015*\"januari\" + 0.014*\"jump\" + 0.014*\"meat\" + 0.013*\"task\"\n",
      "Topic: 32 \n",
      "Words: 0.052*\"video\" + 0.051*\"comedi\" + 0.046*\"huffpost\" + 0.042*\"night\" + 0.025*\"late\" + 0.025*\"watch\" + 0.024*\"twitter\" + 0.017*\"host\" + 0.016*\"week\" + 0.013*\"ladi\"\n",
      "Topic: 33 \n",
      "Words: 0.051*\"state\" + 0.035*\"world\" + 0.030*\"deal\" + 0.026*\"kitchen\" + 0.025*\"unit\" + 0.023*\"pari\" + 0.022*\"global\" + 0.021*\"test\" + 0.021*\"south\" + 0.019*\"french\"\n",
      "Topic: 34 \n",
      "Words: 0.064*\"poll\" + 0.044*\"credit\" + 0.037*\"worri\" + 0.032*\"race\" + 0.030*\"debat\" + 0.026*\"middl\" + 0.023*\"path\" + 0.020*\"polit\" + 0.018*\"candid\" + 0.018*\"east\"\n",
      "Topic: 35 \n",
      "Words: 0.062*\"studi\" + 0.059*\"sleep\" + 0.027*\"diseas\" + 0.024*\"crime\" + 0.024*\"shoot\" + 0.020*\"link\" + 0.018*\"offic\" + 0.015*\"kill\" + 0.014*\"find\" + 0.014*\"especi\"\n",
      "Topic: 36 \n",
      "Words: 0.148*\"style\" + 0.133*\"beauti\" + 0.096*\"photo\" + 0.038*\"look\" + 0.032*\"fashion\" + 0.027*\"week\" + 0.020*\"dress\" + 0.018*\"wear\" + 0.011*\"model\" + 0.011*\"want\"\n",
      "Topic: 37 \n",
      "Words: 0.138*\"wed\" + 0.034*\"babi\" + 0.026*\"coupl\" + 0.025*\"photo\" + 0.024*\"love\" + 0.020*\"marri\" + 0.020*\"video\" + 0.016*\"lose\" + 0.012*\"engag\" + 0.012*\"good\"\n",
      "Topic: 38 \n",
      "Words: 0.060*\"hous\" + 0.056*\"weekend\" + 0.050*\"white\" + 0.049*\"fact\" + 0.033*\"probabl\" + 0.031*\"dinner\" + 0.019*\"breakfast\" + 0.016*\"spous\" + 0.015*\"particular\" + 0.014*\"cheat\"\n",
      "Topic: 39 \n",
      "Words: 0.069*\"heart\" + 0.056*\"relationship\" + 0.033*\"trip\" + 0.025*\"artist\" + 0.021*\"explor\" + 0.020*\"project\" + 0.017*\"fresh\" + 0.017*\"transform\" + 0.016*\"ultim\" + 0.016*\"snack\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['impact', 'number', 'handi', 'intern', 'women', 'face', 'inevit']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.4780633747577667\t \n",
      "Topic: 0.065*\"busi\" + 0.023*\"social\" + 0.021*\"compani\" + 0.021*\"tech\" + 0.018*\"bank\" + 0.015*\"collect\" + 0.014*\"memori\" + 0.014*\"impact\" + 0.013*\"network\" + 0.012*\"develop\"\n",
      "\n",
      "Score: 0.25011584162712097\t \n",
      "Topic: 0.102*\"women\" + 0.046*\"girl\" + 0.024*\"medit\" + 0.020*\"shoe\" + 0.020*\"note\" + 0.019*\"woman\" + 0.018*\"outfit\" + 0.016*\"interest\" + 0.015*\"march\" + 0.014*\"highlight\"\n",
      "\n",
      "Score: 0.15618090331554413\t \n",
      "Topic: 0.029*\"blue\" + 0.027*\"requir\" + 0.023*\"buy\" + 0.018*\"economi\" + 0.018*\"paper\" + 0.016*\"print\" + 0.015*\"januari\" + 0.014*\"jump\" + 0.014*\"meat\" + 0.013*\"task\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.4782792329788208\t \n",
      "Topic: 0.065*\"busi\" + 0.023*\"social\" + 0.021*\"compani\" + 0.021*\"tech\" + 0.018*\"bank\" + 0.015*\"collect\" + 0.014*\"memori\" + 0.014*\"impact\" + 0.013*\"network\" + 0.012*\"develop\"\n",
      "\n",
      "Score: 0.24996843934059143\t \n",
      "Topic: 0.102*\"women\" + 0.046*\"girl\" + 0.024*\"medit\" + 0.020*\"shoe\" + 0.020*\"note\" + 0.019*\"woman\" + 0.018*\"outfit\" + 0.016*\"interest\" + 0.015*\"march\" + 0.014*\"highlight\"\n",
      "\n",
      "Score: 0.1561124622821808\t \n",
      "Topic: 0.029*\"blue\" + 0.027*\"requir\" + 0.023*\"buy\" + 0.018*\"economi\" + 0.018*\"paper\" + 0.016*\"print\" + 0.015*\"januari\" + 0.014*\"jump\" + 0.014*\"meat\" + 0.013*\"task\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.5012917518615723\t Topic: 0.038*\"wall\" + 0.035*\"street\" + 0.035*\"presid\" + 0.032*\"fit\" + 0.030*\"addict\"\n",
      "Score: 0.26119643449783325\t Topic: 0.038*\"manag\" + 0.032*\"card\" + 0.026*\"cocktail\" + 0.024*\"spirit\" + 0.023*\"thanksgiv\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = 'Donald Trump is a loser'\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitter-dash",
   "language": "python",
   "name": "twitter-dash"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
