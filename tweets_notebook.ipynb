{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big thanks to Susan Li for the awesome tutorial on LDA\n",
    "[https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24]\n",
    "\n",
    "This Notebook was inspired by the Medium article above and is written by Advith Chegu for HackDown 2020. Some more sources:\n",
    "\n",
    "[https://medium.com/@osas.usen/topic-extraction-from-tweets-using-lda-a997e4eb0985]\n",
    "\n",
    "[https://www.kaggle.com/therohk/million-headlines/data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/advithchegu/.local/share/virtualenvs/twitter-dash--SggTynj/lib/python3.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "DATASET_COLUMNS  = [\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "DATASET_ENCODING = \"ISO-8859-1\"\n",
    "data = pd.read_json('News_Category_Dataset_v2.json', lines=True)\n",
    "data_text = data[['headline']]\n",
    "data_text.set_index('headline', drop=True, append=False, inplace=False, verify_integrity=False)\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200853\n",
      "                                            headline  index\n",
      "0  There Were 2 Mass Shootings In Texas Last Week...      0\n",
      "1  Will Smith Joins Diplo And Nicky Jam For The 2...      1\n",
      "2    Hugh Grant Marries For The First Time At Age 57      2\n",
      "3  Jim Carrey Blasts 'Castrato' Adam Schiff And D...      3\n",
      "4  Julianna Margulies Uses Donald Trump Poop Bags...      4\n"
     ]
    }
   ],
   "source": [
    "print(len(documents))\n",
    "print(documents[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/advithchegu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "stop_words = ['http']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3 and token not in stop_words and '@' not in token:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['5', 'Numbers', 'To', 'Have', 'Handy', 'When', 'Men', 'Ask', 'Why', 'There', 'Is', 'An', 'International', \"Women's\", 'Day']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['number', 'handi', 'intern', 'women']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = documents[documents['index'] == 4310].values[0][0]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                            [mass, shoot, texa, week]\n",
       "1     [smith, join, diplo, nicki, world, offici, song]\n",
       "2                           [hugh, grant, marri, time]\n",
       "3    [carrey, blast, castrato, adam, schiff, democr...\n",
       "4    [julianna, marguli, use, donald, trump, poop, ...\n",
       "5    [morgan, freeman, devast, sexual, harass, clai...\n",
       "6     [donald, trump, lovin, mcdonald, jingl, tonight]\n",
       "7                         [watch, amazon, prime, week]\n",
       "8    [mike, myer, reveal, like, fourth, austin, pow...\n",
       "9                                  [watch, hulu, week]\n",
       "Name: headline, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = documents['headline'].map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 mass\n",
      "1 shoot\n",
      "2 texa\n",
      "3 week\n",
      "4 diplo\n",
      "5 join\n",
      "6 nicki\n",
      "7 offici\n",
      "8 smith\n",
      "9 song\n",
      "10 world\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(109, 1), (2324, 1), (2719, 1), (4380, 1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 109 (\"women\") appears 1 time.\n",
      "Word 2324 (\"number\") appears 1 time.\n",
      "Word 2719 (\"intern\") appears 1 time.\n",
      "Word 4380 (\"handi\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_4310 = bow_corpus[4310]\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                               dictionary[bow_doc_4310[i][0]], \n",
    "bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.6086420674271222),\n",
      " (1, 0.44082971680772476),\n",
      " (2, 0.534274478977891),\n",
      " (3, 0.38700746200837344)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=40, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LdaModel(num_terms=7260, num_topics=40, decay=0.5, chunksize=2000)\n"
     ]
    }
   ],
   "source": [
    "print(lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.048*\"network\" + 0.045*\"test\" + 0.040*\"green\" + 0.039*\"updat\" + 0.031*\"beach\" + 0.025*\"compani\" + 0.025*\"sale\" + 0.024*\"california\" + 0.023*\"pregnant\" + 0.022*\"photo\"\n",
      "Topic: 1 \n",
      "Words: 0.067*\"learn\" + 0.057*\"worst\" + 0.039*\"cover\" + 0.037*\"social\" + 0.029*\"soul\" + 0.029*\"media\" + 0.027*\"appl\" + 0.023*\"near\" + 0.021*\"biggest\" + 0.020*\"photo\"\n",
      "Topic: 2 \n",
      "Words: 0.189*\"week\" + 0.116*\"parent\" + 0.076*\"women\" + 0.071*\"lose\" + 0.043*\"risk\" + 0.034*\"death\" + 0.026*\"advic\" + 0.024*\"young\" + 0.022*\"best\" + 0.021*\"pictur\"\n",
      "Topic: 3 \n",
      "Words: 0.080*\"better\" + 0.078*\"weight\" + 0.068*\"plan\" + 0.050*\"loss\" + 0.029*\"parti\" + 0.028*\"husband\" + 0.026*\"restaur\" + 0.025*\"win\" + 0.025*\"import\" + 0.024*\"cool\"\n",
      "Topic: 4 \n",
      "Words: 0.052*\"photo\" + 0.040*\"inspir\" + 0.038*\"day\" + 0.034*\"home\" + 0.032*\"long\" + 0.028*\"winter\" + 0.027*\"onlin\" + 0.026*\"video\" + 0.024*\"buy\" + 0.023*\"expert\"\n",
      "Topic: 5 \n",
      "Words: 0.072*\"wear\" + 0.060*\"spring\" + 0.037*\"relationship\" + 0.037*\"huffpost\" + 0.031*\"cloth\" + 0.030*\"challeng\" + 0.028*\"self\" + 0.026*\"room\" + 0.025*\"mistak\" + 0.025*\"name\"\n",
      "Topic: 6 \n",
      "Words: 0.064*\"black\" + 0.042*\"shoot\" + 0.037*\"offic\" + 0.033*\"kill\" + 0.030*\"dead\" + 0.026*\"cocktail\" + 0.026*\"singer\" + 0.025*\"polic\" + 0.025*\"tri\" + 0.025*\"video\"\n",
      "Topic: 7 \n",
      "Words: 0.130*\"health\" + 0.058*\"care\" + 0.037*\"cost\" + 0.037*\"magazin\" + 0.037*\"christma\" + 0.036*\"vacat\" + 0.028*\"john\" + 0.025*\"small\" + 0.024*\"photo\" + 0.020*\"town\"\n",
      "Topic: 8 \n",
      "Words: 0.061*\"bodi\" + 0.058*\"fear\" + 0.054*\"drug\" + 0.036*\"market\" + 0.030*\"launch\" + 0.026*\"practic\" + 0.025*\"price\" + 0.024*\"hard\" + 0.023*\"republican\" + 0.022*\"drop\"\n",
      "Topic: 9 \n",
      "Words: 0.130*\"way\" + 0.050*\"game\" + 0.049*\"kate\" + 0.045*\"night\" + 0.031*\"video\" + 0.030*\"mom\" + 0.026*\"diseas\" + 0.022*\"featur\" + 0.021*\"william\" + 0.019*\"island\"\n",
      "Topic: 10 \n",
      "Words: 0.080*\"right\" + 0.035*\"case\" + 0.033*\"card\" + 0.030*\"fit\" + 0.030*\"cake\" + 0.024*\"line\" + 0.023*\"quiz\" + 0.022*\"team\" + 0.019*\"size\" + 0.018*\"player\"\n",
      "Topic: 11 \n",
      "Words: 0.095*\"sleep\" + 0.085*\"idea\" + 0.039*\"start\" + 0.032*\"futur\" + 0.026*\"thank\" + 0.025*\"danger\" + 0.024*\"skin\" + 0.023*\"medic\" + 0.023*\"juli\" + 0.022*\"treatment\"\n",
      "Topic: 12 \n",
      "Words: 0.056*\"citi\" + 0.039*\"photo\" + 0.036*\"moment\" + 0.036*\"super\" + 0.035*\"york\" + 0.033*\"stay\" + 0.032*\"book\" + 0.027*\"busi\" + 0.024*\"bowl\" + 0.023*\"happen\"\n",
      "Topic: 13 \n",
      "Words: 0.124*\"wed\" + 0.081*\"photo\" + 0.072*\"dress\" + 0.059*\"marriag\" + 0.047*\"famili\" + 0.031*\"gift\" + 0.029*\"step\" + 0.027*\"weekend\" + 0.027*\"stori\" + 0.023*\"question\"\n",
      "Topic: 14 \n",
      "Words: 0.141*\"look\" + 0.097*\"like\" + 0.087*\"find\" + 0.032*\"medit\" + 0.028*\"list\" + 0.022*\"space\" + 0.022*\"park\" + 0.019*\"addict\" + 0.018*\"video\" + 0.017*\"carpet\"\n",
      "Topic: 15 \n",
      "Words: 0.042*\"film\" + 0.035*\"video\" + 0.033*\"festiv\" + 0.030*\"photo\" + 0.030*\"alleg\" + 0.030*\"danc\" + 0.029*\"michael\" + 0.026*\"perform\" + 0.026*\"see\" + 0.023*\"prove\"\n",
      "Topic: 16 \n",
      "Words: 0.079*\"reason\" + 0.036*\"suggest\" + 0.036*\"photo\" + 0.029*\"kati\" + 0.027*\"worth\" + 0.027*\"away\" + 0.026*\"wrong\" + 0.025*\"human\" + 0.024*\"awesom\" + 0.021*\"hospit\"\n",
      "Topic: 17 \n",
      "Words: 0.106*\"love\" + 0.065*\"photo\" + 0.065*\"travel\" + 0.062*\"babi\" + 0.033*\"happi\" + 0.026*\"littl\" + 0.022*\"place\" + 0.021*\"kardashian\" + 0.020*\"workout\" + 0.020*\"trip\"\n",
      "Topic: 18 \n",
      "Words: 0.106*\"recip\" + 0.079*\"photo\" + 0.053*\"hous\" + 0.037*\"white\" + 0.029*\"brain\" + 0.028*\"pound\" + 0.023*\"airlin\" + 0.021*\"decor\" + 0.021*\"fearless\" + 0.021*\"word\"\n",
      "Topic: 19 \n",
      "Words: 0.079*\"celebr\" + 0.065*\"summer\" + 0.056*\"mother\" + 0.054*\"photo\" + 0.049*\"cancer\" + 0.046*\"healthi\" + 0.039*\"friend\" + 0.029*\"father\" + 0.022*\"avoid\" + 0.020*\"simpl\"\n",
      "Topic: 20 \n",
      "Words: 0.091*\"photo\" + 0.063*\"hair\" + 0.061*\"great\" + 0.052*\"high\" + 0.040*\"vintag\" + 0.027*\"shoe\" + 0.025*\"artist\" + 0.024*\"tast\" + 0.021*\"survey\" + 0.020*\"cute\"\n",
      "Topic: 21 \n",
      "Words: 0.146*\"fashion\" + 0.099*\"photo\" + 0.072*\"poll\" + 0.038*\"evolut\" + 0.036*\"success\" + 0.032*\"oscar\" + 0.028*\"lead\" + 0.024*\"york\" + 0.024*\"breast\" + 0.024*\"affect\"\n",
      "Topic: 22 \n",
      "Words: 0.116*\"style\" + 0.097*\"thing\" + 0.064*\"good\" + 0.044*\"photo\" + 0.034*\"season\" + 0.032*\"movi\" + 0.028*\"problem\" + 0.026*\"color\" + 0.023*\"minut\" + 0.021*\"bride\"\n",
      "Topic: 23 \n",
      "Words: 0.145*\"studi\" + 0.072*\"children\" + 0.056*\"school\" + 0.055*\"free\" + 0.048*\"perfect\" + 0.031*\"student\" + 0.030*\"colleg\" + 0.027*\"fail\" + 0.026*\"grow\" + 0.022*\"kid\"\n",
      "Topic: 24 \n",
      "Words: 0.095*\"star\" + 0.069*\"fall\" + 0.043*\"power\" + 0.038*\"mean\" + 0.034*\"facebook\" + 0.033*\"valentin\" + 0.032*\"rais\" + 0.032*\"rise\" + 0.030*\"insid\" + 0.029*\"tour\"\n",
      "Topic: 25 \n",
      "Words: 0.043*\"creat\" + 0.042*\"clean\" + 0.040*\"easi\" + 0.039*\"deal\" + 0.034*\"miss\" + 0.033*\"diet\" + 0.029*\"benefit\" + 0.024*\"credit\" + 0.024*\"keep\" + 0.023*\"issu\"\n",
      "Topic: 26 \n",
      "Words: 0.113*\"know\" + 0.113*\"need\" + 0.065*\"child\" + 0.044*\"favorit\" + 0.041*\"street\" + 0.035*\"rule\" + 0.034*\"wall\" + 0.021*\"court\" + 0.018*\"want\" + 0.016*\"chees\"\n",
      "Topic: 27 \n",
      "Words: 0.079*\"heart\" + 0.073*\"break\" + 0.047*\"leav\" + 0.036*\"london\" + 0.032*\"anti\" + 0.030*\"attack\" + 0.030*\"lgbt\" + 0.028*\"pick\" + 0.023*\"coffe\" + 0.020*\"celeb\"\n",
      "Topic: 28 \n",
      "Words: 0.044*\"cook\" + 0.041*\"exercis\" + 0.029*\"second\" + 0.028*\"patient\" + 0.026*\"west\" + 0.024*\"wine\" + 0.022*\"choic\" + 0.021*\"hand\" + 0.021*\"sport\" + 0.020*\"hurrican\"\n",
      "Topic: 29 \n",
      "Words: 0.115*\"beauti\" + 0.073*\"secret\" + 0.065*\"reveal\" + 0.044*\"photo\" + 0.041*\"news\" + 0.029*\"roundup\" + 0.029*\"financi\" + 0.023*\"move\" + 0.021*\"increas\" + 0.020*\"breakfast\"\n",
      "Topic: 30 \n",
      "Words: 0.054*\"coupl\" + 0.053*\"photo\" + 0.031*\"dream\" + 0.030*\"engag\" + 0.027*\"month\" + 0.027*\"jennif\" + 0.023*\"follow\" + 0.021*\"improv\" + 0.021*\"royal\" + 0.020*\"scienc\"\n",
      "Topic: 31 \n",
      "Words: 0.112*\"obama\" + 0.078*\"trump\" + 0.051*\"presid\" + 0.051*\"visit\" + 0.038*\"surviv\" + 0.037*\"campaign\" + 0.035*\"donald\" + 0.018*\"polit\" + 0.015*\"speech\" + 0.015*\"lie\"\n",
      "Topic: 32 \n",
      "Words: 0.056*\"stress\" + 0.039*\"lesson\" + 0.039*\"eat\" + 0.034*\"shop\" + 0.033*\"person\" + 0.033*\"feel\" + 0.023*\"play\" + 0.023*\"organ\" + 0.021*\"today\" + 0.020*\"photo\"\n",
      "Topic: 33 \n",
      "Words: 0.132*\"tip\" + 0.075*\"holiday\" + 0.074*\"hotel\" + 0.030*\"best\" + 0.029*\"common\" + 0.026*\"product\" + 0.024*\"jessica\" + 0.022*\"prevent\" + 0.021*\"host\" + 0.021*\"video\"\n",
      "Topic: 34 \n",
      "Words: 0.145*\"divorc\" + 0.110*\"food\" + 0.046*\"open\" + 0.045*\"photo\" + 0.030*\"anim\" + 0.029*\"bank\" + 0.026*\"letter\" + 0.021*\"review\" + 0.020*\"store\" + 0.017*\"actress\"\n",
      "Topic: 35 \n",
      "Words: 0.151*\"live\" + 0.059*\"nation\" + 0.046*\"propos\" + 0.044*\"countri\" + 0.028*\"matter\" + 0.026*\"habit\" + 0.023*\"blood\" + 0.022*\"boost\" + 0.022*\"chicken\" + 0.020*\"number\"\n",
      "Topic: 36 \n",
      "Words: 0.112*\"life\" + 0.068*\"chang\" + 0.052*\"photo\" + 0.045*\"save\" + 0.040*\"model\" + 0.034*\"real\" + 0.029*\"drink\" + 0.024*\"differ\" + 0.020*\"pari\" + 0.019*\"natur\"\n",
      "Topic: 37 \n",
      "Words: 0.054*\"think\" + 0.049*\"mind\" + 0.047*\"marri\" + 0.042*\"photo\" + 0.041*\"music\" + 0.037*\"video\" + 0.037*\"surpris\" + 0.036*\"award\" + 0.028*\"sign\" + 0.028*\"memori\"\n",
      "Topic: 38 \n",
      "Words: 0.058*\"money\" + 0.052*\"link\" + 0.045*\"teach\" + 0.031*\"doctor\" + 0.027*\"spend\" + 0.027*\"convers\" + 0.026*\"actual\" + 0.025*\"nail\" + 0.025*\"control\" + 0.024*\"birth\"\n",
      "Topic: 39 \n",
      "Words: 0.072*\"guid\" + 0.071*\"design\" + 0.051*\"date\" + 0.048*\"daughter\" + 0.038*\"video\" + 0.030*\"photo\" + 0.024*\"ladi\" + 0.024*\"walk\" + 0.019*\"vogu\" + 0.017*\"youtub\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['number', 'handi', 'intern', 'women']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.20511244237422943\t \n",
      "Topic: 0.048*\"network\" + 0.045*\"test\" + 0.040*\"green\" + 0.039*\"updat\" + 0.031*\"beach\" + 0.025*\"compani\" + 0.025*\"sale\" + 0.024*\"california\" + 0.023*\"pregnant\" + 0.022*\"photo\"\n",
      "\n",
      "Score: 0.20506323873996735\t \n",
      "Topic: 0.151*\"live\" + 0.059*\"nation\" + 0.046*\"propos\" + 0.044*\"countri\" + 0.028*\"matter\" + 0.026*\"habit\" + 0.023*\"blood\" + 0.022*\"boost\" + 0.022*\"chicken\" + 0.020*\"number\"\n",
      "\n",
      "Score: 0.20502015948295593\t \n",
      "Topic: 0.189*\"week\" + 0.116*\"parent\" + 0.076*\"women\" + 0.071*\"lose\" + 0.043*\"risk\" + 0.034*\"death\" + 0.026*\"advic\" + 0.024*\"young\" + 0.022*\"best\" + 0.021*\"pictur\"\n",
      "\n",
      "Score: 0.20474053919315338\t \n",
      "Topic: 0.079*\"celebr\" + 0.065*\"summer\" + 0.056*\"mother\" + 0.054*\"photo\" + 0.049*\"cancer\" + 0.046*\"healthi\" + 0.039*\"friend\" + 0.029*\"father\" + 0.022*\"avoid\" + 0.020*\"simpl\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.20511186122894287\t \n",
      "Topic: 0.048*\"network\" + 0.045*\"test\" + 0.040*\"green\" + 0.039*\"updat\" + 0.031*\"beach\" + 0.025*\"compani\" + 0.025*\"sale\" + 0.024*\"california\" + 0.023*\"pregnant\" + 0.022*\"photo\"\n",
      "\n",
      "Score: 0.20506268739700317\t \n",
      "Topic: 0.151*\"live\" + 0.059*\"nation\" + 0.046*\"propos\" + 0.044*\"countri\" + 0.028*\"matter\" + 0.026*\"habit\" + 0.023*\"blood\" + 0.022*\"boost\" + 0.022*\"chicken\" + 0.020*\"number\"\n",
      "\n",
      "Score: 0.20501965284347534\t \n",
      "Topic: 0.189*\"week\" + 0.116*\"parent\" + 0.076*\"women\" + 0.071*\"lose\" + 0.043*\"risk\" + 0.034*\"death\" + 0.026*\"advic\" + 0.024*\"young\" + 0.022*\"best\" + 0.021*\"pictur\"\n",
      "\n",
      "Score: 0.2047426849603653\t \n",
      "Topic: 0.079*\"celebr\" + 0.065*\"summer\" + 0.056*\"mother\" + 0.054*\"photo\" + 0.049*\"cancer\" + 0.046*\"healthi\" + 0.039*\"friend\" + 0.029*\"father\" + 0.022*\"avoid\" + 0.020*\"simpl\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.2676692306995392\t Topic: 0.130*\"way\" + 0.050*\"game\" + 0.049*\"kate\" + 0.045*\"night\" + 0.031*\"video\"\n",
      "Score: 0.25623199343681335\t Topic: 0.072*\"guid\" + 0.071*\"design\" + 0.051*\"date\" + 0.048*\"daughter\" + 0.038*\"video\"\n",
      "Score: 0.24483320116996765\t Topic: 0.106*\"love\" + 0.065*\"photo\" + 0.065*\"travel\" + 0.062*\"babi\" + 0.033*\"happi\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = 'That football game was interesting'\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitter-dash",
   "language": "python",
   "name": "twitter-dash"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
